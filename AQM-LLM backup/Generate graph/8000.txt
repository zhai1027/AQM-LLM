root@30265059cf90:/workspace/NetLLM/adaptive_bitrate_streaming# python run_plm.py --adapt --grad-accum-steps 32 --plm-type llama --plm-size base --rank 128 --device cuda:0 --device-out cuda:1 --lr 0.0001 --warmup-steps 2000 --num-epochs 20 --eval-per-epoch 2 --exp-pool-path ./exp_pool.pkl
/usr/local/lib/python3.10/dist-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
Arguments:
Namespace(exp_pool_path='./exp_pool.pkl', sample_step=None, trace='fcc-test', trace_num=100, video='video1', fixed_order=False, plm_type='llama', plm_size='base', rank=128, state_feature_dim=256, w=20, gamma=1.0, lr=0.0001, weight_decay=0.0001, warmup_steps=2000, num_epochs=20, eval_per_epoch=2, save_checkpoint_per_epoch=2, target_return_scale=1.0, which_layer=-1, adapt=True, test=False, grad_accum_steps=32, seed=100003, scale=1000, model_dir=None, device='cuda:0', device_out='cuda:1', device_mid=None)
Loading traces from data/traces/test/fcc-test/
Experience dataset info:
Munch({'max_reward': 12.879270760483154, 'min_reward': 5.174370215368296, 'max_return': 5.0159305295256535, 'min_return': 0.0006744278637263108, 'min_timestep': 0, 'max_timestep': 8000, 'max_action': 3, 'min_action': 0})
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:54<00:00, 27.07s/it]
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
If tokenizer is loaded:  [1, 22172, 3186]

Step 0 - mean train loss  2.185134
Step 100 - mean train loss  1.732931
Step 200 - mean train loss  1.739607
Step 300 - mean train loss  1.734379
==================== Training Iteration #0 ====================
>>>>>>>>>> Training Information:
{'time/training': 68.4677722454071,
 'training/train_loss_mean': 1.726324391067028,
 'training/train_loss_std': 0.21562914126817825}
Checkpoint saved at: data/ft_plms/llama_base/._ss_None/rank_128_w_20_gamma_1.0_sfd_256_lr_0.0001_wd_0.0001_warm_2000_epochs_20_seed_100003/early_stop_-1_checkpoint/0
>>>>>>>>>> Evaluation Information
{'best_return': 0.0,
 'episodes_len': 4700,
 'episodes_return': 0.0,
 'time/evaluation': 820.9913160800934}
Step 0 - mean train loss  1.555751
Step 100 - mean train loss  1.724748
Step 200 - mean train loss  1.698777
Step 300 - mean train loss  1.681826
==================== Training Iteration #1 ====================
>>>>>>>>>> Training Information:
{'time/training': 67.21123909950256,
 'training/train_loss_mean': 1.6573092547059058,
 'training/train_loss_std': 0.22254560160763612}
Step 0 - mean train loss  1.437631
Step 100 - mean train loss  1.579077
Step 200 - mean train loss  1.584850
Step 300 - mean train loss  1.554230
==================== Training Iteration #2 ====================
>>>>>>>>>> Training Information:
{'time/training': 67.78490686416626,
 'training/train_loss_mean': 1.5285049326717854,
 'training/train_loss_std': 0.2413208037720812}
Checkpoint saved at: data/ft_plms/llama_base/._ss_None/rank_128_w_20_gamma_1.0_sfd_256_lr_0.0001_wd_0.0001_warm_2000_epochs_20_seed_100003/early_stop_-1_checkpoint/2
>>>>>>>>>> Evaluation Information
{'best_return': 0.0,
 'episodes_len': 4700,
 'episodes_return': 0.0,
 'time/evaluation': 821.9967024326324}
Step 0 - mean train loss  1.372181
Step 100 - mean train loss  1.469620
Step 200 - mean train loss  1.436364
Step 300 - mean train loss  1.403312
==================== Training Iteration #3 ====================
>>>>>>>>>> Training Information:
{'time/training': 69.83334803581238,
 'training/train_loss_mean': 1.3643455389142036,
 'training/train_loss_std': 0.2615019977153723}
Step 0 - mean train loss  1.123419
Step 100 - mean train loss  1.239483
Step 200 - mean train loss  1.238185
Step 300 - mean train loss  1.198706
==================== Training Iteration #4 ====================
>>>>>>>>>> Training Information:
{'time/training': 70.92360663414001,
 'training/train_loss_mean': 1.1758803230524064,
 'training/train_loss_std': 0.3234542707428894}
Checkpoint saved at: data/ft_plms/llama_base/._ss_None/rank_128_w_20_gamma_1.0_sfd_256_lr_0.0001_wd_0.0001_warm_2000_epochs_20_seed_100003/early_stop_-1_checkpoint/4
>>>>>>>>>> Evaluation Information
{'best_return': 0.0,
 'episodes_len': 4700,
 'episodes_return': 0.0,
 'time/evaluation': 821.6778988838196}
Step 0 - mean train loss  1.064292
Step 100 - mean train loss  1.118950
Step 200 - mean train loss  1.088791
Step 300 - mean train loss  1.038717
==================== Training Iteration #5 ====================
>>>>>>>>>> Training Information:
{'time/training': 61.83467411994934,
 'training/train_loss_mean': 0.9877553158998489,
 'training/train_loss_std': 0.3777820206784268}
Step 0 - mean train loss  0.769825
Step 100 - mean train loss  0.873745
Step 200 - mean train loss  0.873305
Step 300 - mean train loss  0.841325
==================== Training Iteration #6 ====================
>>>>>>>>>> Training Information:
{'time/training': 63.700806617736816,
 'training/train_loss_mean': 0.8538722144812345,
 'training/train_loss_std': 0.550474276861538}
Checkpoint saved at: data/ft_plms/llama_base/._ss_None/rank_128_w_20_gamma_1.0_sfd_256_lr_0.0001_wd_0.0001_warm_2000_epochs_20_seed_100003/early_stop_-1_checkpoint/6
>>>>>>>>>> Evaluation Information
{'best_return': 0.0,
 'episodes_len': 4700,
 'episodes_return': 0.0,
 'time/evaluation': 822.9562544822693}
Step 0 - mean train loss  0.754739
Step 100 - mean train loss  0.902605
Step 200 - mean train loss  0.914124
Step 300 - mean train loss  0.846126
==================== Training Iteration #7 ====================
>>>>>>>>>> Training Information:
{'time/training': 61.23339605331421,
 'training/train_loss_mean': 0.797788933813572,
 'training/train_loss_std': 0.6513844687694412}
Step 0 - mean train loss  0.683727
Step 100 - mean train loss  0.785525
Step 200 - mean train loss  0.792035
Step 300 - mean train loss  0.769258
==================== Training Iteration #8 ====================
>>>>>>>>>> Training Information:
{'time/training': 61.03956413269043,
 'training/train_loss_mean': 0.8135613339021802,
 'training/train_loss_std': 0.8698840585955396}
Checkpoint saved at: data/ft_plms/llama_base/._ss_None/rank_128_w_20_gamma_1.0_sfd_256_lr_0.0001_wd_0.0001_warm_2000_epochs_20_seed_100003/early_stop_-1_checkpoint/8
>>>>>>>>>> Evaluation Information
{'best_return': 0.0,
 'episodes_len': 4700,
 'episodes_return': 0.0,
 'time/evaluation': 822.2361736297607}
Step 0 - mean train loss  0.704735
Step 100 - mean train loss  0.955437
Step 200 - mean train loss  0.963734
Step 300 - mean train loss  0.873191
==================== Training Iteration #9 ====================
>>>>>>>>>> Training Information:
{'time/training': 66.46084642410278,
 'training/train_loss_mean': 0.8149227782338858,
 'training/train_loss_std': 0.8769680314310655}
Step 0 - mean train loss  0.694838
Step 100 - mean train loss  0.791333
Step 200 - mean train loss  0.788094
Step 300 - mean train loss  0.764954
==================== Training Iteration #10 ====================
>>>>>>>>>> Training Information:
{'time/training': 68.40717554092407,
 'training/train_loss_mean': 0.8101747009158135,
 'training/train_loss_std': 0.9062539299631555}
Checkpoint saved at: data/ft_plms/llama_base/._ss_None/rank_128_w_20_gamma_1.0_sfd_256_lr_0.0001_wd_0.0001_warm_2000_epochs_20_seed_100003/early_stop_-1_checkpoint/10
>>>>>>>>>> Evaluation Information
{'best_return': 0.0,
 'episodes_len': 4700,
 'episodes_return': 0.0,
 'time/evaluation': 822.3059985637665}
Step 0 - mean train loss  0.692886
Step 100 - mean train loss  0.951284
Step 200 - mean train loss  0.949341
Step 300 - mean train loss  0.859517
==================== Training Iteration #11 ====================
>>>>>>>>>> Training Information:
{'time/training': 71.03565740585327,
 'training/train_loss_mean': 0.8072771659493446,
 'training/train_loss_std': 0.8856392728987398}
Step 0 - mean train loss  0.733400
Step 100 - mean train loss  0.784569
Step 200 - mean train loss  0.783054
Step 300 - mean train loss  0.756923
==================== Training Iteration #12 ====================
>>>>>>>>>> Training Information:
{'time/training': 71.54159951210022,
 'training/train_loss_mean': 0.8005543180182576,
 'training/train_loss_std': 0.8926670460264883}
Checkpoint saved at: data/ft_plms/llama_base/._ss_None/rank_128_w_20_gamma_1.0_sfd_256_lr_0.0001_wd_0.0001_warm_2000_epochs_20_seed_100003/early_stop_-1_checkpoint/12
>>>>>>>>>> Evaluation Information
{'best_return': 0.0,
 'episodes_len': 4700,
 'episodes_return': 0.0,
 'time/evaluation': 821.45205950737}
Step 0 - mean train loss  0.693550
Step 100 - mean train loss  0.949019
Step 200 - mean train loss  0.940922
Step 300 - mean train loss  0.851522
==================== Training Iteration #13 ====================
>>>>>>>>>> Training Information:
{'time/training': 69.10368013381958,
 'training/train_loss_mean': 0.7997390814125538,
 'training/train_loss_std': 0.8691685758596702}
Step 0 - mean train loss  0.741502
Step 100 - mean train loss  0.787106
Step 200 - mean train loss  0.776587
Step 300 - mean train loss  0.751683
==================== Training Iteration #14 ====================
>>>>>>>>>> Training Information:
{'time/training': 65.71217393875122,
 'training/train_loss_mean': 0.7947561353072524,
 'training/train_loss_std': 0.8727319657945336}
Checkpoint saved at: data/ft_plms/llama_base/._ss_None/rank_128_w_20_gamma_1.0_sfd_256_lr_0.0001_wd_0.0001_warm_2000_epochs_20_seed_100003/early_stop_-1_checkpoint/14
>>>>>>>>>> Evaluation Information
{'best_return': 0.0,
 'episodes_len': 4700,
 'episodes_return': 0.0,
 'time/evaluation': 820.7321424484253}
Step 0 - mean train loss  0.674685
Step 100 - mean train loss  0.969220
Step 200 - mean train loss  0.947737
Step 300 - mean train loss  0.859274
==================== Training Iteration #15 ====================
>>>>>>>>>> Training Information:
{'time/training': 66.94067811965942,
 'training/train_loss_mean': 0.8135579833760858,
 'training/train_loss_std': 0.8639381748056731}
Step 0 - mean train loss  0.760928
Step 100 - mean train loss  0.776362
Step 200 - mean train loss  0.776952
Step 300 - mean train loss  0.750764
==================== Training Iteration #16 ====================
>>>>>>>>>> Training Information:
{'time/training': 67.35540580749512,
 'training/train_loss_mean': 0.7940604793652892,
 'training/train_loss_std': 0.8729276425000879}
Checkpoint saved at: data/ft_plms/llama_base/._ss_None/rank_128_w_20_gamma_1.0_sfd_256_lr_0.0001_wd_0.0001_warm_2000_epochs_20_seed_100003/early_stop_-1_checkpoint/16
>>>>>>>>>> Evaluation Information
{'best_return': 0.0,
 'episodes_len': 4700,
 'episodes_return': 0.0,
 'time/evaluation': 820.4318406581879}
Step 0 - mean train loss  0.679174
Step 100 - mean train loss  0.961640
Step 200 - mean train loss  0.939349
Step 300 - mean train loss  0.848797
==================== Training Iteration #17 ====================
>>>>>>>>>> Training Information:
{'time/training': 67.25060081481934,
 'training/train_loss_mean': 0.7950260419398546,
 'training/train_loss_std': 0.8454700587751905}
Step 0 - mean train loss  0.757615
Step 100 - mean train loss  0.776047
Step 200 - mean train loss  0.768485
Step 300 - mean train loss  0.741263
==================== Training Iteration #18 ====================
>>>>>>>>>> Training Information:
{'time/training': 68.2051727771759,
 'training/train_loss_mean': 0.7815117066912353,
 'training/train_loss_std': 0.8438489684094073}
Checkpoint saved at: data/ft_plms/llama_base/._ss_None/rank_128_w_20_gamma_1.0_sfd_256_lr_0.0001_wd_0.0001_warm_2000_epochs_20_seed_100003/early_stop_-1_checkpoint/18
>>>>>>>>>> Evaluation Information
{'best_return': 0.0,
 'episodes_len': 4700,
 'episodes_return': 0.0,
 'time/evaluation': 820.3017456531525}
Step 0 - mean train loss  0.671294
Step 100 - mean train loss  0.951962
Step 200 - mean train loss  0.921764
Step 300 - mean train loss  0.838964
==================== Training Iteration #19 ====================
>>>>>>>>>> Training Information:
{'time/training': 70.70480680465698,
 'training/train_loss_mean': 0.7888521325960756,
 'training/train_loss_std': 0.8168278468015958}