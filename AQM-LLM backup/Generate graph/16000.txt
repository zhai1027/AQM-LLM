root@30265059cf90:/workspace/NetLLM/adaptive_bitrate_streaming# python run_plm.py --adapt --grad-accum-steps 32 --plm-type llama --plm-size base --rank 128 --device cuda:0 --device-out cuda:1 --lr 0.0001 --warmup-steps 2000 --num-epochs 20 --eval-per-epoch 2 --exp-pool-path ./exp_pool.pkl
/usr/local/lib/python3.10/dist-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
Arguments:
Namespace(exp_pool_path='./exp_pool.pkl', sample_step=None, trace='fcc-test', trace_num=100, video='video1', fixed_order=False, plm_type='llama', plm_size='base', rank=128, state_feature_dim=256, w=20, gamma=1.0, lr=0.0001, weight_decay=0.0001, warmup_steps=2000, num_epochs=20, eval_per_epoch=2, save_checkpoint_per_epoch=2, target_return_scale=1.0, which_layer=-1, adapt=True, test=False, grad_accum_steps=32, seed=100003, scale=1000, model_dir=None, device='cuda:0', device_out='cuda:1', device_mid=None)
Loading traces from data/traces/test/fcc-test/
Experience dataset info:
Munch({'max_reward': 12.879270760483154, 'min_reward': 5.174370215368296, 'max_return': 10.159953091323167, 'min_return': 0.0007409973711550864, 'min_timestep': 0, 'max_timestep': 16000, 'max_action': 3, 'min_action': 0})
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:59<00:00, 29.53s/it]
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
If tokenizer is loaded:  [1, 22172, 3186]

Step 0 - mean train loss  1.869575
Step 100 - mean train loss  1.919954
Step 200 - mean train loss  1.929864
Step 300 - mean train loss  1.921349
Step 400 - mean train loss  1.907284
Step 500 - mean train loss  1.903027
Step 600 - mean train loss  1.895194
Step 700 - mean train loss  1.892020
==================== Training Iteration #0 ====================
>>>>>>>>>> Training Information:
{'time/training': 139.18895530700684,
 'training/train_loss_mean': 1.8942026549577713,
 'training/train_loss_std': 0.25985075840908795}
Checkpoint saved at: data/ft_plms/llama_base/._ss_None/rank_128_w_20_gamma_1.0_sfd_256_lr_0.0001_wd_0.0001_warm_2000_epochs_20_seed_100003/early_stop_-1_checkpoint/0
>>>>>>>>>> Evaluation Information
{'best_return': 0.0,
 'episodes_len': 4700,
 'episodes_return': 0.0,
 'time/evaluation': 822.4404287338257}
Step 0 - mean train loss  1.832699
Step 100 - mean train loss  1.859480
Step 200 - mean train loss  1.855246
Step 300 - mean train loss  1.824449
Step 400 - mean train loss  1.805212
Step 500 - mean train loss  1.784934
Step 600 - mean train loss  1.776026
Step 700 - mean train loss  1.758225
==================== Training Iteration #1 ====================
>>>>>>>>>> Training Information:
{'time/training': 137.2878975868225,
 'training/train_loss_mean': 1.7356807047128677,
 'training/train_loss_std': 0.2791071926340852}
Step 0 - mean train loss  1.732553
Step 100 - mean train loss  1.532712
Step 200 - mean train loss  1.517285
Step 300 - mean train loss  1.514572
Step 400 - mean train loss  1.501251
Step 500 - mean train loss  1.475571
Step 600 - mean train loss  1.450155
Step 700 - mean train loss  1.429067
==================== Training Iteration #2 ====================
>>>>>>>>>> Training Information:
{'time/training': 135.6487729549408,
 'training/train_loss_mean': 1.4007669270783663,
 'training/train_loss_std': 0.3157310300526871}
Checkpoint saved at: data/ft_plms/llama_base/._ss_None/rank_128_w_20_gamma_1.0_sfd_256_lr_0.0001_wd_0.0001_warm_2000_epochs_20_seed_100003/early_stop_-1_checkpoint/2
>>>>>>>>>> Evaluation Information
{'best_return': 0.0,
 'episodes_len': 4700,
 'episodes_return': 0.0,
 'time/evaluation': 823.9388768672943}
Step 0 - mean train loss  1.182109
Step 100 - mean train loss  1.138444
Step 200 - mean train loss  1.107248
Step 300 - mean train loss  1.054929
Step 400 - mean train loss  1.013366
Step 500 - mean train loss  0.975646
Step 600 - mean train loss  0.956650
Step 700 - mean train loss  0.923681
==================== Training Iteration #3 ====================
>>>>>>>>>> Training Information:
{'time/training': 138.0896303653717,
 'training/train_loss_mean': 0.9012858072668314,
 'training/train_loss_std': 0.4711117345692926}
Step 0 - mean train loss  0.680796
Step 100 - mean train loss  0.691032
Step 200 - mean train loss  0.763817
Step 300 - mean train loss  0.762642
Step 400 - mean train loss  0.776337
Step 500 - mean train loss  0.753311
Step 600 - mean train loss  0.750002
Step 700 - mean train loss  0.738542
==================== Training Iteration #4 ====================
>>>>>>>>>> Training Information:
{'time/training': 138.82201647758484,
 'training/train_loss_mean': 0.750450660660863,
 'training/train_loss_std': 0.8176116702142728}
Checkpoint saved at: data/ft_plms/llama_base/._ss_None/rank_128_w_20_gamma_1.0_sfd_256_lr_0.0001_wd_0.0001_warm_2000_epochs_20_seed_100003/early_stop_-1_checkpoint/4
>>>>>>>>>> Evaluation Information
{'best_return': 0.0,
 'episodes_len': 4700,
 'episodes_return': 0.0,
 'time/evaluation': 824.1209154129028}
Step 0 - mean train loss  0.606724
Step 100 - mean train loss  0.667568
Step 200 - mean train loss  0.691378
Step 300 - mean train loss  0.707561
Step 400 - mean train loss  0.714327
Step 500 - mean train loss  0.725959
Step 600 - mean train loss  0.757638
Step 700 - mean train loss  0.754869
==================== Training Iteration #5 ====================
>>>>>>>>>> Training Information:
{'time/training': 137.6411566734314,
 'training/train_loss_mean': 0.756573957875371,
 'training/train_loss_std': 0.8941359272028152}
Step 0 - mean train loss  0.700327
Step 100 - mean train loss  0.697798
Step 200 - mean train loss  0.777857
Step 300 - mean train loss  0.773891
Step 400 - mean train loss  0.783140
Step 500 - mean train loss  0.756837
Step 600 - mean train loss  0.752000
Step 700 - mean train loss  0.739484
==================== Training Iteration #6 ====================
>>>>>>>>>> Training Information:
{'time/training': 139.41292762756348,
 'training/train_loss_mean': 0.7504787151701748,
 'training/train_loss_std': 0.8855221350625693}
Checkpoint saved at: data/ft_plms/llama_base/._ss_None/rank_128_w_20_gamma_1.0_sfd_256_lr_0.0001_wd_0.0001_warm_2000_epochs_20_seed_100003/early_stop_-1_checkpoint/6
>>>>>>>>>> Evaluation Information
{'best_return': 0.0,
 'episodes_len': 4700,
 'episodes_return': 0.0,
 'time/evaluation': 824.4589130878448}
Step 0 - mean train loss  0.609420
Step 100 - mean train loss  0.657910
Step 200 - mean train loss  0.670225
Step 300 - mean train loss  0.685623
Step 400 - mean train loss  0.695207
Step 500 - mean train loss  0.709225
Step 600 - mean train loss  0.743538
Step 700 - mean train loss  0.742284
==================== Training Iteration #7 ====================
>>>>>>>>>> Training Information:
{'time/training': 138.16075205802917,
 'training/train_loss_mean': 0.7441176716797053,
 'training/train_loss_std': 0.8616952534973352}
Step 0 - mean train loss  0.675571
Step 100 - mean train loss  0.691542
Step 200 - mean train loss  0.765745
Step 300 - mean train loss  0.762355
Step 400 - mean train loss  0.768537
Step 500 - mean train loss  0.744158
Step 600 - mean train loss  0.739877
Step 700 - mean train loss  0.728971
==================== Training Iteration #8 ====================
>>>>>>>>>> Training Information:
{'time/training': 135.99957728385925,
 'training/train_loss_mean': 0.739197271745652,
 'training/train_loss_std': 0.8248455352032876}
Checkpoint saved at: data/ft_plms/llama_base/._ss_None/rank_128_w_20_gamma_1.0_sfd_256_lr_0.0001_wd_0.0001_warm_2000_epochs_20_seed_100003/early_stop_-1_checkpoint/8
>>>>>>>>>> Evaluation Information
{'best_return': 0.0,
 'episodes_len': 4700,
 'episodes_return': 0.0,
 'time/evaluation': 825.4149034023285}
Step 0 - mean train loss  0.609353
Step 100 - mean train loss  0.658520
Step 200 - mean train loss  0.674963
Step 300 - mean train loss  0.686963
Step 400 - mean train loss  0.695171
Step 500 - mean train loss  0.707446
Step 600 - mean train loss  0.739807
Step 700 - mean train loss  0.737248
==================== Training Iteration #9 ====================
>>>>>>>>>> Training Information:
{'time/training': 138.70508360862732,
 'training/train_loss_mean': 0.7383715672791005,
 'training/train_loss_std': 0.8009827925452111}
Step 0 - mean train loss  0.666410
Step 100 - mean train loss  0.681295
Step 200 - mean train loss  0.748924
Step 300 - mean train loss  0.745558
Step 400 - mean train loss  0.752459
Step 500 - mean train loss  0.731273
Step 600 - mean train loss  0.728086
Step 700 - mean train loss  0.718590
==================== Training Iteration #10 ====================
>>>>>>>>>> Training Information:
{'time/training': 138.94975996017456,
 'training/train_loss_mean': 0.7283131210878492,
 'training/train_loss_std': 0.7557355434046341}
Checkpoint saved at: data/ft_plms/llama_base/._ss_None/rank_128_w_20_gamma_1.0_sfd_256_lr_0.0001_wd_0.0001_warm_2000_epochs_20_seed_100003/early_stop_-1_checkpoint/10
>>>>>>>>>> Evaluation Information
{'best_return': 0.0,
 'episodes_len': 4700,
 'episodes_return': 0.0,
 'time/evaluation': 825.0959787368774}
Step 0 - mean train loss  0.615358
Step 100 - mean train loss  0.657150
Step 200 - mean train loss  0.662185
Step 300 - mean train loss  0.675090
Step 400 - mean train loss  0.685765
Step 500 - mean train loss  0.699346
Step 600 - mean train loss  0.733052
Step 700 - mean train loss  0.731270
==================== Training Iteration #11 ====================
>>>>>>>>>> Training Information:
{'time/training': 136.05985140800476,
 'training/train_loss_mean': 0.7339708325639367,
 'training/train_loss_std': 0.7980209968079163}
Step 0 - mean train loss  0.676480
Step 100 - mean train loss  0.682294
Step 200 - mean train loss  0.753173
Step 300 - mean train loss  0.745304
Step 400 - mean train loss  0.750680
Step 500 - mean train loss  0.731825
Step 600 - mean train loss  0.736023
Step 700 - mean train loss  0.727296
==================== Training Iteration #12 ====================
>>>>>>>>>> Training Information:
{'time/training': 133.77851557731628,
 'training/train_loss_mean': 0.7404933406412602,
 'training/train_loss_std': 0.7867307303255021}
Checkpoint saved at: data/ft_plms/llama_base/._ss_None/rank_128_w_20_gamma_1.0_sfd_256_lr_0.0001_wd_0.0001_warm_2000_epochs_20_seed_100003/early_stop_-1_checkpoint/12
>>>>>>>>>> Evaluation Information
{'best_return': 0.0,
 'episodes_len': 4700,
 'episodes_return': 0.0,
 'time/evaluation': 823.7727136611938}
Step 0 - mean train loss  0.621421
Step 100 - mean train loss  0.662342
Step 200 - mean train loss  0.674008
Step 300 - mean train loss  0.690877
Step 400 - mean train loss  0.701468
Step 500 - mean train loss  0.710743
Step 600 - mean train loss  0.739249
Step 700 - mean train loss  0.736920
==================== Training Iteration #13 ====================
>>>>>>>>>> Training Information:
{'time/training': 130.18808770179749,
 'training/train_loss_mean': 0.7380809105560183,
 'training/train_loss_std': 0.7893101592300416}
Step 0 - mean train loss  0.662274
Step 100 - mean train loss  0.684440
Step 200 - mean train loss  0.753420
Step 300 - mean train loss  0.747052
Step 400 - mean train loss  0.752529
Step 500 - mean train loss  0.732653
Step 600 - mean train loss  0.736009
Step 700 - mean train loss  0.727957
==================== Training Iteration #14 ====================
>>>>>>>>>> Training Information:
{'time/training': 132.93151473999023,
 'training/train_loss_mean': 0.740684210266918,
 'training/train_loss_std': 0.7792093575619731}
Checkpoint saved at: data/ft_plms/llama_base/._ss_None/rank_128_w_20_gamma_1.0_sfd_256_lr_0.0001_wd_0.0001_warm_2000_epochs_20_seed_100003/early_stop_-1_checkpoint/14
>>>>>>>>>> Evaluation Information
{'best_return': 0.0,
 'episodes_len': 4700,
 'episodes_return': 0.0,
 'time/evaluation': 824.4153761863708}
Step 0 - mean train loss  0.628690
Step 100 - mean train loss  0.678323
Step 200 - mean train loss  0.677860
Step 300 - mean train loss  0.698656
Step 400 - mean train loss  0.720089
Step 500 - mean train loss  0.727741
Step 600 - mean train loss  0.748141
Step 700 - mean train loss  0.742348
==================== Training Iteration #15 ====================
>>>>>>>>>> Training Information:
{'time/training': 130.95742893218994,
 'training/train_loss_mean': 0.7429088635370136,
 'training/train_loss_std': 0.7304538617648396}
Step 0 - mean train loss  0.702314
Step 100 - mean train loss  0.693171
Step 200 - mean train loss  0.752511
Step 300 - mean train loss  0.744077
Step 400 - mean train loss  0.753719
Step 500 - mean train loss  0.734096
Step 600 - mean train loss  0.736996
Step 700 - mean train loss  0.726078
==================== Training Iteration #16 ====================
>>>>>>>>>> Training Information:
{'time/training': 130.71513557434082,
 'training/train_loss_mean': 0.7378149378485978,
 'training/train_loss_std': 0.8369559074851528}
Checkpoint saved at: data/ft_plms/llama_base/._ss_None/rank_128_w_20_gamma_1.0_sfd_256_lr_0.0001_wd_0.0001_warm_2000_epochs_20_seed_100003/early_stop_-1_checkpoint/16
>>>>>>>>>> Evaluation Information
{'best_return': 0.0,
 'episodes_len': 4700,
 'episodes_return': 0.0,
 'time/evaluation': 823.6950268745422}
Step 0 - mean train loss  0.607356
Step 100 - mean train loss  0.653199
Step 200 - mean train loss  0.658905
Step 300 - mean train loss  0.675842
Step 400 - mean train loss  0.693856
Step 500 - mean train loss  0.709025
Step 600 - mean train loss  0.742824
Step 700 - mean train loss  0.742149
==================== Training Iteration #17 ====================
>>>>>>>>>> Training Information:
{'time/training': 139.48322987556458,
 'training/train_loss_mean': 0.7440824967250228,
 'training/train_loss_std': 0.9120365076281748}
Step 0 - mean train loss  0.666506
Step 100 - mean train loss  0.691088
Step 200 - mean train loss  0.770228
Step 300 - mean train loss  0.760394
Step 400 - mean train loss  0.767489
Step 500 - mean train loss  0.750107
Step 600 - mean train loss  0.771823
Step 700 - mean train loss  0.771667
==================== Training Iteration #18 ====================
>>>>>>>>>> Training Information:
{'time/training': 142.9976954460144,
 'training/train_loss_mean': 0.7744594382680953,
 'training/train_loss_std': 0.8505467470570535}
Checkpoint saved at: data/ft_plms/llama_base/._ss_None/rank_128_w_20_gamma_1.0_sfd_256_lr_0.0001_wd_0.0001_warm_2000_epochs_20_seed_100003/early_stop_-1_checkpoint/18
>>>>>>>>>> Evaluation Information
{'best_return': 0.0,
 'episodes_len': 4700,
 'episodes_return': 0.0,
 'time/evaluation': 821.8006296157837}
Step 0 - mean train loss  0.623783
Step 100 - mean train loss  0.688130
Step 200 - mean train loss  0.677849
Step 300 - mean train loss  0.686737
Step 400 - mean train loss  0.705040
Step 500 - mean train loss  0.713678
Step 600 - mean train loss  0.734347
Step 700 - mean train loss  0.730457
==================== Training Iteration #19 ====================
>>>>>>>>>> Training Information:
{'time/training': 141.1927511692047,
 'training/train_loss_mean': 0.7346506796590984,
 'training/train_loss_std': 0.6889586362494559}s