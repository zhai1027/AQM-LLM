root@30265059cf90:/workspace/NetLLM/adaptive_bitrate_streaming# python run_plm.py --adapt --grad-accum-steps 32 --plm-type llama --plm-size base --rank 128 --device cuda:0 --device-out cuda:1 --lr 0.0001 --warmup-steps 2000 --num-epochs 20 --eval-per-epoch 2 --exp-pool-path ./exp_pool.pkl/usr/local/lib/python3.10/dist-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
Arguments:
Namespace(exp_pool_path='./exp_pool.pkl', sample_step=None, trace='fcc-test', trace_num=100, video='video1', fixed_order=False, plm_type='llama', plm_size='base', rank=128, state_feature_dim=256, w=20, gamma=1.0, lr=0.0001, weight_decay=0.0001, warmup_steps=2000, num_epochs=20, eval_per_epoch=2, save_checkpoint_per_epoch=2, target_return_scale=1.0, which_layer=-1, adapt=True, test=False, grad_accum_steps=32, seed=100003, scale=1000, model_dir=None, device='cuda:0', device_out='cuda:1', device_mid=None)
Loading traces from data/traces/test/fcc-test/
Experience dataset info:
Munch({'max_reward': 12.879270760483154, 'min_reward': 5.174370215368296, 'max_return': 1.1892910152724983, 'min_return': 0.0006124067117802709, 'min_timestep': 0, 'max_timestep': 2000, 'max_action': 3, 'min_action': 0})
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:49<00:00, 24.98s/it]
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
If tokenizer is loaded:  [1, 22172, 3186]

Step 0 - mean train loss  1.498260
==================== Training Iteration #0 ====================
>>>>>>>>>> Training Information:
{'time/training': 18.14624261856079,
 'training/train_loss_mean': 2.4693753290176392,
 'training/train_loss_std': 0.9197729932597875}
Checkpoint saved at: data/ft_plms/llama_base/._ss_None/rank_128_w_20_gamma_1.0_sfd_256_lr_0.0001_wd_0.0001_warm_2000_epochs_20_seed_100003/early_stop_-1_checkpoint/0
>>>>>>>>>> Evaluation Information
{'best_return': 0.0,
 'episodes_len': 4700,
 'episodes_return': 0.0,
 'time/evaluation': 816.3093392848969}
Step 0 - mean train loss  3.503922
==================== Training Iteration #1 ====================
>>>>>>>>>> Training Information:
{'time/training': 17.998600482940674,
 'training/train_loss_mean': 2.4662275242805483,
 'training/train_loss_std': 0.9190685787727915}
Step 0 - mean train loss  1.447321
==================== Training Iteration #2 ====================
>>>>>>>>>> Training Information:
{'time/training': 17.89397096633911,
 'training/train_loss_mean': 2.459587417840958,
 'training/train_loss_std': 0.9176607846642514}
Checkpoint saved at: data/ft_plms/llama_base/._ss_None/rank_128_w_20_gamma_1.0_sfd_256_lr_0.0001_wd_0.0001_warm_2000_epochs_20_seed_100003/early_stop_-1_checkpoint/2
>>>>>>>>>> Evaluation Information
{'best_return': 0.0,
 'episodes_len': 4700,
 'episodes_return': 0.0,
 'time/evaluation': 817.9297358989716}
Step 0 - mean train loss  3.486032
==================== Training Iteration #3 ====================
>>>>>>>>>> Training Information:
{'time/training': 18.000765085220337,
 'training/train_loss_mean': 2.4492561519145966,
 'training/train_loss_std': 0.9164037978034993}
Step 0 - mean train loss  1.422207
==================== Training Iteration #4 ====================
>>>>>>>>>> Training Information:
{'time/training': 17.912442922592163,
 'training/train_loss_mean': 2.43556276679039,
 'training/train_loss_std': 0.9141379504305553}
Checkpoint saved at: data/ft_plms/llama_base/._ss_None/rank_128_w_20_gamma_1.0_sfd_256_lr_0.0001_wd_0.0001_warm_2000_epochs_20_seed_100003/early_stop_-1_checkpoint/4
>>>>>>>>>> Evaluation Information
{'best_return': 0.0,
 'episodes_len': 4700,
 'episodes_return': 0.0,
 'time/evaluation': 818.3349716663361}
Step 0 - mean train loss  3.453073
==================== Training Iteration #5 ====================
>>>>>>>>>> Training Information:
{'time/training': 17.806260347366333,
 'training/train_loss_mean': 2.4182612001895905,
 'training/train_loss_std': 0.9124081554084773}
Step 0 - mean train loss  1.380639
==================== Training Iteration #6 ====================
>>>>>>>>>> Training Information:
{'time/training': 18.0174617767334,
 'training/train_loss_mean': 2.397838053703308,
 'training/train_loss_std': 0.9093058192000882}
Checkpoint saved at: data/ft_plms/llama_base/._ss_None/rank_128_w_20_gamma_1.0_sfd_256_lr_0.0001_wd_0.0001_warm_2000_epochs_20_seed_100003/early_stop_-1_checkpoint/6
>>>>>>>>>> Evaluation Information
{'best_return': 0.0,
 'episodes_len': 4700,
 'episodes_return': 0.0,
 'time/evaluation': 817.7259523868561}
Step 0 - mean train loss  3.405495
==================== Training Iteration #7 ====================
>>>>>>>>>> Training Information:
{'time/training': 17.939403533935547,
 'training/train_loss_mean': 2.3739272487163543,
 'training/train_loss_std': 0.9071847969643252}
Step 0 - mean train loss  1.323564
==================== Training Iteration #8 ====================
>>>>>>>>>> Training Information:
{'time/training': 18.005481004714966,
 'training/train_loss_mean': 2.347171006202698,
 'training/train_loss_std': 0.9033408801668009}
Checkpoint saved at: data/ft_plms/llama_base/._ss_None/rank_128_w_20_gamma_1.0_sfd_256_lr_0.0001_wd_0.0001_warm_2000_epochs_20_seed_100003/early_stop_-1_checkpoint/8
>>>>>>>>>> Evaluation Information
{'best_return': 0.0,
 'episodes_len': 4700,
 'episodes_return': 0.0,
 'time/evaluation': 819.2815585136414}
Step 0 - mean train loss  3.343940
==================== Training Iteration #9 ====================
>>>>>>>>>> Training Information:
{'time/training': 18.243306159973145,
 'training/train_loss_mean': 2.3170748889446258,
 'training/train_loss_std': 0.9009624835939746}
Step 0 - mean train loss  1.252023
==================== Training Iteration #10 ====================
>>>>>>>>>> Training Information:
{'time/training': 18.20461392402649,
 'training/train_loss_mean': 2.2844322431087494,
 'training/train_loss_std': 0.8965301188082317}
Checkpoint saved at: data/ft_plms/llama_base/._ss_None/rank_128_w_20_gamma_1.0_sfd_256_lr_0.0001_wd_0.0001_warm_2000_epochs_20_seed_100003/early_stop_-1_checkpoint/10
>>>>>>>>>> Evaluation Information
{'best_return': 0.0,
 'episodes_len': 4700,
 'episodes_return': 0.0,
 'time/evaluation': 819.3118553161621}
Step 0 - mean train loss  3.269166
==================== Training Iteration #11 ====================
>>>>>>>>>> Training Information:
{'time/training': 18.0434730052948,
 'training/train_loss_mean': 2.2486189115047455,
 'training/train_loss_std': 0.8940814989692819}
Step 0 - mean train loss  1.167130
==================== Training Iteration #12 ====================
>>>>>>>>>> Training Information:
{'time/training': 17.991148710250854,
 'training/train_loss_mean': 2.210578063726425,
 'training/train_loss_std': 0.8892743611193427}
Checkpoint saved at: data/ft_plms/llama_base/._ss_None/rank_128_w_20_gamma_1.0_sfd_256_lr_0.0001_wd_0.0001_warm_2000_epochs_20_seed_100003/early_stop_-1_checkpoint/12
>>>>>>>>>> Evaluation Information
{'best_return': 0.0,
 'episodes_len': 4700,
 'episodes_return': 0.0,
 'time/evaluation': 817.9302814006805}
Step 0 - mean train loss  3.182000
==================== Training Iteration #13 ====================
>>>>>>>>>> Training Information:
{'time/training': 17.602283477783203,
 'training/train_loss_mean': 2.1695601743459703,
 'training/train_loss_std': 0.8870052089281809}
Step 0 - mean train loss  1.070100
==================== Training Iteration #14 ====================
>>>>>>>>>> Training Information:
{'time/training': 17.746514081954956,
 'training/train_loss_mean': 2.126661731600761,
 'training/train_loss_std': 0.8821167572820274}
Checkpoint saved at: data/ft_plms/llama_base/._ss_None/rank_128_w_20_gamma_1.0_sfd_256_lr_0.0001_wd_0.0001_warm_2000_epochs_20_seed_100003/early_stop_-1_checkpoint/14
>>>>>>>>>> Evaluation Information
{'best_return': 0.0,
 'episodes_len': 4700,
 'episodes_return': 0.0,
 'time/evaluation': 817.2593972682953}
Step 0 - mean train loss  3.083335
==================== Training Iteration #15 ====================
>>>>>>>>>> Training Information:
{'time/training': 17.78309726715088,
 'training/train_loss_mean': 2.081019043326378,
 'training/train_loss_std': 0.8803670926599614}
Step 0 - mean train loss  0.962312
==================== Training Iteration #16 ====================
>>>>>>>>>> Training Information:
{'time/training': 18.135953903198242,
 'training/train_loss_mean': 2.0338952744007113,
 'training/train_loss_std': 0.8758193248425141}
Checkpoint saved at: data/ft_plms/llama_base/._ss_None/rank_128_w_20_gamma_1.0_sfd_256_lr_0.0001_wd_0.0001_warm_2000_epochs_20_seed_100003/early_stop_-1_checkpoint/16
>>>>>>>>>> Evaluation Information
{'best_return': 0.0,
 'episodes_len': 4700,
 'episodes_return': 0.0,
 'time/evaluation': 817.3995320796967}
Step 0 - mean train loss  2.974242
==================== Training Iteration #17 ====================
>>>>>>>>>> Training Information:
{'time/training': 17.942072868347168,
 'training/train_loss_mean': 1.9843317484855652,
 'training/train_loss_std': 0.8750869447714972}
Step 0 - mean train loss  0.845421
==================== Training Iteration #18 ====================
>>>>>>>>>> Training Information:
{'time/training': 17.817686080932617,
 'training/train_loss_mean': 1.9337951809167861,
 'training/train_loss_std': 0.8715432943228095}
Checkpoint saved at: data/ft_plms/llama_base/._ss_None/rank_128_w_20_gamma_1.0_sfd_256_lr_0.0001_wd_0.0001_warm_2000_epochs_20_seed_100003/early_stop_-1_checkpoint/18
>>>>>>>>>> Evaluation Information
{'best_return': 0.0,
 'episodes_len': 4700,
 'episodes_return': 0.0,
 'time/evaluation': 817.7542684078217}
Step 0 - mean train loss  2.856384
==================== Training Iteration #19 ====================
>>>>>>>>>> Training Information:
{'time/training': 17.921968698501587,
 'training/train_loss_mean': 1.8812452924251557,
 'training/train_loss_std': 0.8726252471210013}